{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display, Math,  Latex\n",
    "matplotlib.style.use('ggplot')\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from tqdm import *\n",
    "\n",
    "def classifier_crossval_performance(x, Y, classifier=GaussianNB(), n_folds=10, weights=None, bins=50):\n",
    "    #print some empty lines\n",
    "    print(\"\\n \\n\")\n",
    "    \n",
    "    #create axis and figure\n",
    "    fig,(ax, ax2, ax3) = plt.subplots(3 , 1)\n",
    "    fig.set_size_inches(12, 18)\n",
    "    #create inset axis for zooming\n",
    "    axins = zoomed_inset_axes(ax, 3.5, loc=1)\n",
    "    \n",
    "    labels_predctions = []\n",
    "\n",
    "\n",
    "    #save all aucs and confusion matrices for each cv fold\n",
    "    roc_aucs = []\n",
    "    confusion_matrices = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f_scores = []\n",
    "\n",
    "    #iterate over test and training sets \n",
    "    cv = cross_validation.StratifiedKFold(y, n_folds=n_folds)\n",
    "    test_weights = None\n",
    "    \n",
    "    for train, test in tqdm(cv):\n",
    "        #select data\n",
    "        xtrain, xtest = x[train], x[test]\n",
    "        ytrain, ytest = Y[train], Y[test]\n",
    "        \n",
    "        #fit and predict \n",
    "        classifier.fit(xtrain, ytrain)\n",
    "        y_probas = classifier.predict_proba(xtest)[:,1]\n",
    "        y_prediction = classifier.predict(xtest)\n",
    "        labels_predctions.append((ytest, y_prediction, y_probas))\n",
    "    \n",
    "    #calculate metrics\n",
    "    #save all aucs and confusion matrices for each cv fold\n",
    "    roc_aucs = np.zeros(n_folds)\n",
    "    confusion_matrices = np.zeros([n_folds,2,2])\n",
    "    precisions = np.zeros(n_folds)\n",
    "    recalls = np.zeros(n_folds)\n",
    "    f_scores = np.zeros(n_folds)\n",
    "\n",
    "    for i, (test, prediction, proba) in enumerate(labels_predctions):\n",
    "        matrix = metrics.confusion_matrix(test, prediction)\n",
    "        p,r,f,_ = metrics.precision_recall_fscore_support(test, prediction, sample_weight=test_weights)\n",
    "        auc = metrics.roc_auc_score(test, proba, sample_weight=test_weights)\n",
    "        \n",
    "        confusion_matrices[i] = matrix\n",
    "        roc_aucs[i] = auc\n",
    "        precisions[i] = p[1]\n",
    "        recalls[i] = r[1]\n",
    "        f_scores[i] = f[1]\n",
    "\n",
    "        \n",
    "    #plot roc aucs \n",
    "    for test, prediction, proba in labels_predctions:\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test, proba, sample_weight=test_weights)\n",
    "        ax.plot(fpr, tpr, linestyle=\"-\", color=\"0.4\")\n",
    "        axins.plot(fpr, tpr, linestyle=\"-\", color=\"0.4\")\n",
    "\n",
    "    #plot stuff with confidence cuts\n",
    "    matrices = np.zeros((n_folds, bins, 2, 2))\n",
    "    for fold, (test, prediction, probas) in enumerate(labels_predctions):    \n",
    "        for i, cut in enumerate(np.linspace(0,1,bins)):\n",
    "            cutted_prediction = prediction.copy()\n",
    "            cutted_prediction[probas < cut] = 0\n",
    "            cutted_prediction[probas >= cut] = 1\n",
    "            confusion = metrics.confusion_matrix(test, cutted_prediction)\n",
    "            matrices[fold][i] = confusion         \n",
    "\n",
    "    \n",
    "    b = np.linspace(0,1,bins)\n",
    "    \n",
    "    tps = matrices[:,:,0, 0]\n",
    "    fns = matrices[:,:,0, 1]\n",
    "    fps = matrices[:,:,1, 0]\n",
    "    tns = matrices[:,:,1, 1]\n",
    "\n",
    "    q_mean = np.mean(tps / np.sqrt(tns), axis=0)\n",
    "    q_err = np.std(tps / np.sqrt(tns), axis=0)\n",
    "    e_mean = np.mean(tps / np.sqrt(tns + tps), axis=0)\n",
    "    e_err = np.std(tps / np.sqrt(tns + tps), axis=0)\n",
    "    \n",
    "    ax2.plot(b, q_mean , color=\"blue\", linestyle=\"\", marker=\"+\", label=r\"$\\frac{tps}{\\sqrt{tns}}$\")\n",
    "    ax2.plot(b, e_mean , color=\"#58BADB\", linestyle=\"\", marker=\"+\", label=r\"$\\frac{tps}{\\sqrt{tns + tps}}$\")\n",
    "    ax2.fill_between(b, q_mean + q_err*0.5, q_mean - q_err*0.5, facecolor='gray', alpha=0.4)\n",
    "    ax2.fill_between(b, e_mean + e_err*0.5, e_mean - e_err*0.5, facecolor='gray', alpha=0.4)\n",
    "    ax2.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "    ax2.set_xlabel(\"prediction threshold\")\n",
    "    \n",
    "    accs = (tps + tns)/(tps + fps + fns + tns)\n",
    "    acc_mean = np.mean(accs, axis=0)\n",
    "    acc_err = np.std(accs, axis=0)\n",
    "    \n",
    "    ax3.plot(b, acc_mean , color=\"red\", linestyle=\"\", marker=\"+\", label=r\"Accuracy\")\n",
    "    ax3.fill_between(b, acc_mean + acc_err*0.5, acc_mean - acc_err*0.5, facecolor='gray', alpha=0.4)\n",
    "    ax3.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "    ax3.set_xlabel(\"prediction threshold\")\n",
    "   \n",
    "\n",
    "    ax.set_xlabel(\"False Positiv Rate\")\n",
    "    ax.set_ylabel(\"True Positiv Rate\")\n",
    "    ax.set_ylim(0.0,1.0)\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    axins.set_xlim(0, 0.15)\n",
    "    axins.set_ylim(0.8, 1.0)\n",
    "    axins.set_xticks([0.0, 0.05, 0.1, 0.15])\n",
    "    axins.set_yticks([0.8, 0.85, 0.9, 0.95, 1.0])\n",
    "    mark_inset(ax, axins, loc1=2, loc2=3, fc=\"none\", ec=\"0.8\")\n",
    "    name = str(type(classifier).__name__)\n",
    "    ax.set_title(\"RoC curves for the {} classifier\".format(name), y=1.03)\n",
    "    \n",
    "    print_performance(roc_aucs ,confusion_matrices,precisions,recalls, f_scores)\n",
    "    plt.show()\n",
    "    return roc_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_performance(roc_aucs ,confusion_matrices,precisions,recalls, f_scores):\n",
    "\n",
    "    tp = confusion_matrices[:, 0 , 0]\n",
    "    fn = confusion_matrices[:, 0 , 1]\n",
    "    fp = confusion_matrices[:, 1 , 0]\n",
    "    tn = confusion_matrices[:, 1 , 1]\n",
    "    \n",
    "    \n",
    "    l = r\"\"\"\\begin{matrix}\n",
    "      {:.2f} \\pm {:.2f} & {:.2f} \\pm {:.2f} \\\\\n",
    "      {:.2f} \\pm {:.2f} & {:.2f} \\pm {:.2f} \n",
    "     \\end{matrix}\"\"\".format(tp.mean(),tp.std(), fn.mean(),fn.std(),fp.mean(),fp.std(), tn.mean(), tn.std(), matrix=\"{matrix}\")\n",
    "\n",
    "    print(\"Confusion matrix: \")\n",
    "    display(Latex(l))\n",
    "    print()\n",
    "    \n",
    "    fpr = fp/(fp + tn)\n",
    "    relative_error = (fpr.std() / fpr.mean() )*100\n",
    "    print(\"Mean False Positive Rate:: \")\n",
    "    l = r\"\"\"{:.5f} \\pm {:.5f}  \\qquad (\\pm {:.1f} \\quad \\%)\"\"\".format(fpr.mean(), fpr.std(), relative_error)\n",
    "\n",
    "    display(Math(l))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    print(\"Mean area under ROC curve: \")\n",
    "    relative_error = (roc_aucs.std() / roc_aucs.mean() )*100\n",
    "    l = r\"\"\"{:.5f} \\pm {:.5f} \\qquad (\\pm {:.1f} \\quad \\%)\"\"\".format(roc_aucs.mean(), roc_aucs.std(), relative_error)\n",
    "    display(Math(l))\n",
    "    print()\n",
    "    \n",
    "    #print(\"Mean precission:\")\n",
    "    #p_mean = perf_dict[\"precision\"].mean()\n",
    "    #p_error = perf_dict[\"precision\"].std()/np.sqrt(len(perf_dict[\"precision\"]))\n",
    "    #l = r\"\"\"{:.5f} \\pm {:.5f}\"\"\".format(p_mean, p_error)\n",
    "    #display(Math(l))\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "    print(\"Mean recall:\")\n",
    "    relative_error = (recalls.std() / recalls.mean() )*100\n",
    "    l = r\"\"\"{:.5f} \\pm {:.5f}\\qquad (\\pm {:.1f} \\quad \\%)\"\"\".format(recalls.mean(), recalls.std(), relative_error)\n",
    "    display(Math(l))\n",
    "    print()\n",
    "    \n",
    "    print(\"Mean fscore:\")\n",
    "    relative_error = (f_scores.std() / f_scores.mean() )*100\n",
    "    l = r\"\"\"{:.5f} \\pm {:.5f}\\qquad (\\pm {:.1f} \\quad \\%)\"\"\".format(f_scores.mean(), f_scores.std(), relative_error)\n",
    "    display(Math(l))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn import calibration\n",
    "\n",
    "def plot_recall_precission_curve(y, y_prediction, weights =  None, bins=50):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, y_prediction, sample_weight=weights)\n",
    "    #print(metrics.accuracy_score(y_test, gnb.predict(X_test)))\n",
    "    \n",
    "    fraction_of_positives, mean_predicted_value = calibration.calibration_curve(y, y_prediction, n_bins=bins)\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(13,5))\n",
    "    ax1.hist(y_prediction, bins=bins)\n",
    "    ax1.set_title(\"Histogram of predicted probabilities\")\n",
    "    ax1.set_xlabel(\"Probabilities\")\n",
    "    #print(precision, recall)\n",
    "    \n",
    "    \n",
    "    ax2.plot(recall, precision, label=\"Recall /TPR\", linestyle=\"-\")\n",
    "    ax2.set_xlabel(\"Recall\")\n",
    "    ax2.set_ylabel(\"Precision\")\n",
    "    #ax.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "    #ax2.legend()\n",
    "    ax2.set_title(\"Precision/Recall curve\")\n",
    "    \n",
    "    ax3.plot(mean_predicted_value, fraction_of_positives)\n",
    "    ax3.plot([0,1], [0,1], color=\"gray\", linestyle=\"--\")\n",
    "    ax3.set_xlabel(\"Mean Predicted Value\")\n",
    "    ax3.set_ylabel(\"Fraction of positives\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load signal \n",
    "df_signal = pd.read_csv(\"signal.csv\", sep=\";\")\n",
    "df_signal.dropna(axis=[1,0], how='all', inplace=True)\n",
    "print(\"Number of signal Features: {}. Label {}\".format(len(df_signal.columns), df_signal.label.iloc[0]))\n",
    "\n",
    "#load background\n",
    "df_background = pd.read_csv(\"background.csv\", sep=\";\")\n",
    "df_background.dropna(axis=[1,0], how='all', inplace=True)\n",
    "print(\"Number of background features: {}. Label: {}\".format(len(df_background.columns), df_background.label.iloc[0]))\n",
    "\n",
    "#lets only take columns that appear in both datasets\n",
    "df = pd.concat([df_signal, df_background], axis=0, join='inner')\n",
    "\n",
    "#match  all columns containing the name 'corsika'\n",
    "c = df.filter(regex=\"((C|c)orsika)(\\w*[\\._\\-]\\w*)?\").columns\n",
    "df = df.drop(c, axis=1)\n",
    "\n",
    "#match any column containing header, MC, utc, mjd, date or ID. Im sure there are better regexes for this.\n",
    "c = df.filter(regex=\"\\w*[\\.\\-_]*(((u|U)(t|T)(c|C))|(MC)|((m|M)(j|J)(d|D))|(Weight)|((h|H)eader)|((d|D)ate)|(ID))+\\w*[\\.\\-_]*\\w*\").columns\n",
    "df = df.drop(c, axis=1)\n",
    "\n",
    "#drop columns containing only a single value\n",
    "df = df.drop(df.var()[df.var() == 0].index, axis=1)\n",
    "#some features are weird. Pearsons r is NaN. Better drop those columns\n",
    "#corr = df.apply(lambda c: stats.pearsonr(c, df.label)[0])\n",
    "#df = df.drop(corr[corr.isnull()].index, axis=1)\n",
    "#print(df.columns)\n",
    "print(\"Combined Features: {}\".format(len(df.columns)))\n",
    "\n",
    "#df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data weights and Signal/Background ratio\n",
    "\n",
    "Each data sample comes with a weight. Usually IceCube MonteCarlos are simulated with a $E^{-1}$ spectrum. At first we sum up all the weights for each class and divide them to get the ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal_weight = df_signal[\"Weight.HoSa\"].sum()\n",
    "background_weight = df_background[\"Weight.HoSa\"].sum()\n",
    "print(\"Signal Weight: \")\n",
    "print(signal_weight)\n",
    "\n",
    "print(\"Background Weight: \")\n",
    "print(background_weight)\n",
    "\n",
    "print(\"Ratio:\")\n",
    "ratio = background_weight/signal_weight\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a little low. Dataset needs to be reweighted. Lets suppose the real ratio should be something like $1 : 10000$. Then we have to multiply the background weights by  $10000 / 382.388$ and create a new array for sample weights. These have to be normalize to the [0,1] range for use with sklearn as far as I understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factor = 10000/ ratio\n",
    "df_background[\"Weight.HoSa\"] *= factor\n",
    "background_weight = df_background[\"Weight.HoSa\"].sum()\n",
    "ratio = background_weight/signal_weight\n",
    "\n",
    "l = Latex(\"Signal to background ratio: $1 : {}$\".format(ratio))\n",
    "display(l)\n",
    "\n",
    "#normalize\n",
    "sample_weights = np.append(df_background[\"Weight.HoSa\"].values, df_signal[\"Weight.HoSa\"].values)\n",
    "sample_weights /= np.abs(sample_weights.max() - sample_weights.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't quite figured out how to use these weights for evaluating classfiers within sklearn. Feeding these to performance metric functions does nothing. I'm not going to use them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gaussian Naive Bayes Classifier\n",
    "\n",
    "Lets try the Gaussian Naive Bayes algorithm first. It assumes that the values of the features are distributed according to a Gaussian. To quote wikipedia:\n",
    "\n",
    "  > Let $\\mu_c$ be the mean of the values in $x$ associated with class $c$, and let $\\sigma^2_c$ be the variance of the \n",
    "  > values in $x$ associated with class $c$. Then, the probability distribution of some value given a class,\n",
    "  > $p(x=v|c)$, can be computed by plugging v into the equation for a Normal distribution parameterized by\n",
    "  > $\\mu_c$ and $\\sigma^2_c$. That is,\n",
    "  > $$p(x=v|c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_c}}\\,e^{ -\\frac{(v-\\mu_c)^2}{2\\sigma^2_c} } $$\n",
    "  \n",
    "Feature scaling or mean shifting is not needed for NaiveBayes. It would have no effect. All NaNs and Infs have to removed however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "df_nb_label = df.dropna(axis=1)[\"label\"]\n",
    "df_nb = df.dropna(axis=1).drop(\"label\", axis=1)\n",
    "print(\"There are {} remaining features after dropping columns containing NaNs.\".format(len(df_nb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_aucs = classifier_crossval_perormance(df_nb.values, df_nb_label.values, classifier=gnb, weights=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test ,_, weights_test= cross_validation.train_test_split(df_nb, df_nb_label, sample_weights, test_size=0.33)\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "y_prediction = gnb.predict_proba(X_test)[:,1]\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Forest Clasifier\n",
    "\n",
    "The usual ensemble method used by everyone. Scaling is not necessary as we use *Information Gain* to select features in each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf = ensemble.RandomForestClassifier(n_jobs=48, n_estimators=48, criterion=\"entropy\")\n",
    "X = df.dropna(axis=1).drop(\"label\", axis=1).values\n",
    "y = df.dropna(axis=1)[\"label\"].values\n",
    "rf_aucs = classifier_crossval_perormance(X, y, classifier=rf, bins=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, _, weights_test = cross_validation.train_test_split(X, y, sample_weights, test_size=0.33)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prediction = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction, weights=None, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration\n",
    "Calibrate prediction using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_sigmoid = calibration.CalibratedClassifierCV(rf, cv=10, method='sigmoid')\n",
    "rf_sigmoid.fit(X_train, y_train)\n",
    "y_prediction = rf_sigmoid.predict_proba(X_test)[:,1]\n",
    "#print( metrics.roc_auc_score(y_test,y_prediction ))\n",
    "#print( metrics.roc_auc_score(y_test,y_prediction, sample_weight=weights_test ))\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction, weights=None, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate and check the new calibrated classfier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibrated_rf_aucs = classifier_crossval_perormance(X, y, classifier=rf_sigmoid, bins=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the classification does not get better after calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extremly Randomized Trees\n",
    "\n",
    "This randomizes even more than a random forests. Instead of calculating the best split according to the given criterion, it creates a number of random splits and selects the best of those. In my opinion this should be much faster on continous features. Yay!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "extra_rf = ensemble.ExtraTreesClassifier(n_jobs=24, n_estimators=2*24, criterion=\"entropy\")\n",
    "X = df.dropna(axis=1).drop(\"label\", axis=1).values\n",
    "y = df.dropna(axis=1)[\"label\"].values\n",
    "erf_aucs = classifier_crossval_perormance(X, y, classifier=extra_rf, bins=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, _, weights_test = cross_validation.train_test_split(X, y, sample_weights, test_size=0.33)\n",
    "extra_rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = extra_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extra_rf_sigmoid = calibration.CalibratedClassifierCV(extra_rf, cv=10, method='sigmoid')\n",
    "extra_rf_sigmoid.fit(X_train, y_train)\n",
    "y_prediction = extra_rf_sigmoid.predict_proba(X_test)[:,1]\n",
    "#print( metrics.roc_auc_score(y_test,y_prediction ))\n",
    "#print( metrics.roc_auc_score(y_test,y_prediction, sample_weight=weights_test ))\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction, weights=None, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check calibrated classfier. Takes too long. Im impatient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibrated_erf_aucs = classifier_crossval_perormance(X, y, classifier=extra_rf_sigmoid, bins=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Boosting Classifier\n",
    "Use small decission trees (or stumps) in a boosted ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = ensemble.GradientBoostingClassifier(n_estimators=10, max_depth=3)\n",
    "X = df.dropna(axis=1).drop(\"label\", axis=1).values\n",
    "y = df.dropna(axis=1)[\"label\"].values\n",
    "grb_aucs  = classifier_crossval_perormance(X, y, classifier=gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= cross_validation.train_test_split(X, y, test_size=0.33)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = gbc.predict_proba(X_test)[:,1]\n",
    "\n",
    "plot_recall_precission_curve(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparing classifier performances\n",
    "\n",
    "Assuming the values calculated during the cross validation are normaly distributed around a fixed mean we can use a welch test to determine whether the number differ significantly. Here we use Welches t-test for the nullhypothesis of identical sample means with different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from itertools import product\n",
    "def welch_test(a, b, significance = 0.05):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    _, p = stats.ttest_ind(a, b, equal_var = False)\n",
    "    if p < significance :\n",
    "        print(\"Null hypothesis rejected with p: {}. ROC AUC differs significantly\".format(p))\n",
    "    else:\n",
    "        print(\"Null hypothesis cannot be rejected with p: {}\".format(p))\n",
    "\n",
    "print(\"Compare Naive Bayes to Random Forest\")\n",
    "welch_test(nb_aucs, rf_aucs)\n",
    "\n",
    "print(\"Compare Random Forest to Extremly Random Forest\")\n",
    "welch_test(rf_aucs, erf_aucs)\n",
    "\n",
    "print(\"Compare Extremly Random Forest to Gradient Boosting Classifier\")\n",
    "welch_test(erf_aucs, grb_aucs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Selection\n",
    "\n",
    "Use Logistic regression to find good features. This might have an impact on the performance of the NaiveBayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.dropna(axis=1).drop(\"label\", axis=1)\n",
    "y = df.dropna(axis=1)[\"label\"]\n",
    "\n",
    "rfe = sklearn.feature_selection.RFE(LogisticRegression() , 10, step = 25)\n",
    "X_sel = rfe.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected features: \")\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_extra_rf = ensemble.ExtraTreesClassifier(n_jobs=24, n_estimators=2*24, criterion=\"entropy\")\n",
    "fs_erf_aucs = classifier_crossval_perormance(X_sel, y.values, classifier=fs_extra_rf, bins=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Compare GNB to Features Slected GNB\")\n",
    "welch_test(fs_erf_aucs, erf_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
